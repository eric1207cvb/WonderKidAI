import Foundation
import Speech
import AVFoundation
import AudioToolbox

enum SpeechPermissionState {
    case authorized
    case denied
    case notDetermined
}

class SpeechService: NSObject {
    static let shared = SpeechService()
    
    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // å›èª¿
    var onSpeechDetected: ((String, Bool) -> Void)?
    var onRecordingStarted: (() -> Void)?
    
    private var silenceTimer: Timer?
    private let silenceThreshold: TimeInterval = 1.5
    
    override private init() {
        super.init()
        // åˆå§‹åŒ–æ™‚é è¨­èªè¨€
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW"))
    }

    func permissionState() -> SpeechPermissionState {
        let speechStatus = SFSpeechRecognizer.authorizationStatus()
        let recordStatus = AVAudioSession.sharedInstance().recordPermission
        
        if speechStatus == .authorized && recordStatus == .granted {
            return .authorized
        }
        
        if speechStatus == .denied || speechStatus == .restricted || recordStatus == .denied {
            return .denied
        }
        
        return .notDetermined
    }

    func requestPermissions(completion: @escaping (Bool) -> Void) {
        let group = DispatchGroup()
        var speechGranted = false
        var recordGranted = false
        
        group.enter()
        SFSpeechRecognizer.requestAuthorization { status in
            speechGranted = (status == .authorized)
            group.leave()
        }
        
        group.enter()
        AVAudioSession.sharedInstance().requestRecordPermission { granted in
            recordGranted = granted
            group.leave()
        }
        
        group.notify(queue: .main) {
            completion(speechGranted && recordGranted)
        }
    }
    
    // MARK: - ğŸ”¥ æ ¸å¿ƒä¿®æ­£ï¼šçµ±ä¸€çš„ AudioSession è¨­å®š
    // (é€™å°±æ˜¯ Xcode èªªæ‰¾ä¸åˆ°çš„é‚£å€‹åŠŸèƒ½ï¼Œç¾åœ¨è£œä¸Šäº†ï¼)
    func configureAudioSession(isRecording: Bool) {
        do {
            let session = AVAudioSession.sharedInstance()
            if isRecording {
                // éŒ„éŸ³æ¨¡å¼ï¼šåŒæ™‚å…è¨±æ’­æ”¾èˆ‡éŒ„éŸ³ï¼Œä¸¦å¼·åˆ¶è²éŸ³å¾å–‡å­å‡ºä¾†
                try session.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker, .allowBluetoothHFP])
            } else {
                // æ’­æ”¾æ¨¡å¼ï¼šå°ˆæ³¨æ–¼æ’­æ”¾
                try session.setCategory(.playback, mode: .default)
            }
            try session.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            print("âŒ Audio Session è¨­å®šå¤±æ•—: \(error)")
        }
    }
    
    func startRecording(language: AppLanguage) throws {
        stopRecording() // å…ˆç¢ºä¿ä¹‹å‰çš„æ¸…ç†ä¹¾æ·¨
        
        // 1. è¨­å®šéŸ³è¨Šç’°å¢ƒ
        configureAudioSession(isRecording: true)
        
        #if DEBUG
        let authStatus = SFSpeechRecognizer.authorizationStatus()
        print("[STT] authStatus=\(authStatus.rawValue)")
        #endif
        
        // 2. æ’­æ”¾æç¤ºéŸ³ (1113: Begin Recording)
        AudioServicesPlaySystemSound(1113)
        
        // ğŸ‡¯ğŸ‡µ æ”¯æ´ä¸‰ç¨®èªè¨€
        let localeID: String
        switch language {
        case .chinese:
            localeID = "zh-TW"
        case .english:
            localeID = "en-US"
        case .japanese:
            localeID = "ja-JP"
        }
        
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: localeID))
        #if DEBUG
        if speechRecognizer?.isAvailable == false {
            print("[STT] recognizer unavailable for locale=\(localeID)")
        }
        #endif
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        recognitionRequest.shouldReportPartialResults = true
        
        let inputNode = audioEngine.inputNode
        
        // æ¨¡æ“¬å™¨é˜²å‘†èˆ‡æ ¼å¼è¨­å®š
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        var formatToUse = recordingFormat
        if recordingFormat.sampleRate == 0 {
            if let fallbackFormat = AVAudioFormat(standardFormatWithSampleRate: 44100, channels: 1) {
                formatToUse = fallbackFormat
            }
        }
        
        // 3. è¨­å®šè¾¨è­˜ä»»å‹™
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            var isFinal = false
            
            if let result = result {
                let text = result.bestTranscription.formattedString
                // ç¢ºä¿å›èª¿åœ¨ä¸»ç·šç¨‹
                DispatchQueue.main.async {
                    self.onSpeechDetected?(text, false)
                    self.resetSilenceTimer()
                }
                isFinal = result.isFinal
            }
            
            if let error = error {
                #if DEBUG
                print("[STT] recognition error: \(error)")
                #endif
            }
            
            if error != nil || isFinal {
                self.stopRecording()
            }
        }
        
        inputNode.removeTap(onBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: formatToUse) { (buffer, _) in
            recognitionRequest.append(buffer)
        }
        
        audioEngine.prepare()
        try audioEngine.start()
        
        print("ğŸ™ï¸ éº¥å…‹é¢¨å·²å•Ÿå‹•")
        DispatchQueue.main.async {
            self.onRecordingStarted?()
        }
    }
    
    func stopRecording() {
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
            AudioServicesPlaySystemSound(1114) // End Recording Sound
        }
        
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil // é‡‹æ”¾è«‹æ±‚
        
        print("ğŸ›‘ éŒ„éŸ³çµæŸ")
    }
    
    private func resetSilenceTimer() {
        silenceTimer?.invalidate()
        DispatchQueue.main.async {
            self.silenceTimer = Timer.scheduledTimer(withTimeInterval: self.silenceThreshold, repeats: false) { [weak self] _ in
                // æ™‚é–“åˆ°ï¼Œè¦–ç‚ºä¸€å¥è©±çµæŸ (True)
                self?.onSpeechDetected?("", true)
                self?.stopRecording()
            }
        }
    }
    
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { _ in }
        AVAudioSession.sharedInstance().requestRecordPermission { _ in }
    }
}
